{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "import os\n",
    "from importlib import reload\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.debugger import Pdb; ipdb = Pdb()\n",
    "from heudiconv import utils, parser\n",
    "\n",
    "# don't really need dask (especially distributed),\n",
    "#  it can be useful for converting larger datasets though\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import delayed\n",
    "\n",
    "import heudiconv_helpers.helpers # this is a module so can be easily reloaded upon modification\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    \"\"\"Run a string that represents a command.\n",
    "    Generating command strings and executing them is \n",
    "    a quick and dirty way of flexibly generating commands\n",
    "    in python and executing them on different compute\n",
    "    infrastructures.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    pp = subprocess.run(cmd,shell=True,stdout=subprocess.PIPE,stderr= subprocess.PIPE)\n",
    "    print([v.split('//')[-1] for v in pp.stderr.decode('utf-8').splitlines() ])\n",
    "    return pp\n",
    "\n",
    "# Set pandas display settings\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2 # EDIT: number of workers to use with Dask.\n",
    "client = Client(processes = False,\n",
    "    n_workers= n_workers,\n",
    "    threads_per_worker=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an version string to append to filenames in order to distinguish\n",
    "# between different analyses.\n",
    "analysis_version = \"2018_08\" # EDIT\n",
    "\n",
    "# Define a project directory. If all analysis file and\n",
    "# data are container within this directory then all files\n",
    "# can be defined relative to this. This setup makes it easier\n",
    "# to handle paths and to move the project between hosts as required\n",
    "project_dir_absolute = Path('.').absolute() # EDIT: needs to be pathlib.Path object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell changes to the notebooks ipython kernel\n",
    "# to the project directory. This enables tab completion\n",
    "# of all paths within the project which makes things\n",
    "# go more smoothly and avoids errors.\n",
    "%pwd\n",
    "%cd {project_dir_absolute.as_posix()}\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell allows lots of customization. In general, the only things\n",
    "# that are useful to change are marked with the \"EDIT\" tag. The rest is \n",
    "# defined here to increase transparency but should work if left alone.\n",
    "\n",
    "# Define the directory to search through to gather dicoms for the \n",
    "# conversion\n",
    "dicoms_dir = Path('data/dicoms') # EDIT and place dicoms at this location\n",
    "\n",
    "scripts_dir  = Path('analysis_files/heudiconv_files')\n",
    "\n",
    "# Define the path to the heudiconv heuristics script. This script enables one\n",
    "# to map from dicom metadata to the bids directory specification using some rules\n",
    "# written in python\n",
    "heuristics_script = scripts_dir.joinpath('heuristics/heuristics_file_' + analysis_version + '.py')\n",
    "\n",
    "# Specify the path to a software container image with heudiconv. This can be \n",
    "# found on docker hub at nipy/heudiconv. If a conversion to singularity is \n",
    "# required you can use https://github.com/singularityware/docker2singularity\n",
    "sing_image = scripts_dir.joinpath('path_to_heudiconv_image.img')\n",
    "\n",
    "# If not using compressed dicom directories a slightly differnt syntax\n",
    "# is required for heudiconv\n",
    "dicom_extension = '.tar.gz' # EDIT\n",
    "\n",
    "# When heudiconv is run with a software container, the project \n",
    "# directory is mounted into the container at the mount point /data.\n",
    "#  The output is written to outdir (a path starting with /data), which\n",
    "# is defined below:\n",
    "outdir = Path(\"/data/bids_\" + analysis_version)\n",
    "\n",
    "# The output of heudiconv will have a different path on the local filesystem:\n",
    "output_of_heudiconv = Path(outdir.as_posix().replace('/data/','')) \n",
    "\n",
    "# In order to define the heuristics for heudiconv to use on a dataset,\n",
    "# a generic run of heudiconv is typically required. This uses a default\n",
    "# heuristics file that captures all scans and extracts their metadata.\n",
    "# This metadata is essential to define the mapping as described later.\n",
    "outdir_gen = Path(outdir.as_posix() + '_generic')\n",
    "output_of_heudiconv_gen = Path(outdir_gen.as_posix().replace('/data/',''))\n",
    "\n",
    "\n",
    "logdir = scripts_dir.joinpath('conversion_logs')\n",
    "symlinked_dicoms = dicoms_dir.absolute().with_name('symlinked_dicoms')\n",
    "\n",
    "\n",
    "# Recursively create necessary directories for analysis files.\n",
    "for directory in [symlinked_dicoms,scripts_dir,logdir]:\n",
    "    if not directory.exists():\n",
    "        os.makedirs(directory,exist_ok=True)\n",
    "\n",
    "        \n",
    "# Create a dictionary to keep track of the different versions of\n",
    "# a conversion. If it already exists in memory it is just written\n",
    "# to disk:\n",
    "conversion_dict_pkld = scripts_dir.joinpath('conversion_dict.pklz')\n",
    "\n",
    "if 'conversion_dict' in locals():\n",
    "    conversion_dict.to_pickle(conversion_dict_pkld)\n",
    "else:\n",
    "    if conversion_dict_pkld.exists():\n",
    "        conversion_dict = pd.read_pickle(conversion_dict_pkld)\n",
    "    else:\n",
    "        conversion_dict = pd.Series({})\n",
    "\n",
    "# The path specified below is used to mount the heudiconv source code\n",
    "# into the software container to allow a more conveneient method of\n",
    "# debugging any problems.\n",
    "heudiconv_source_code = \"path_to_heudiconv_source_code\" # EDIT if debugging is required\n",
    "    \n",
    "# Define a path used to a tsv that is created by the notebook to generate\n",
    "# an arbitrary shift of all acquisition dates for each subject\n",
    "acq_date_time_offset = logdir.with_name('acquisition_date_time_offset.tsv')\n",
    "\n",
    "# If available specify a key to map patient ids specified in the dicom\n",
    "# filenames to an anomyized id\n",
    "patient_key_path = scripts_dir.joinpath('patient_key.pklz') # EDIT\n",
    "\n",
    "# A path to record the mapping once heudiconv is run\n",
    "info_mapping_csv = scripts_dir.joinpath(f'seqinfo_mapping_{analysis_version}.csv')\n",
    "\n",
    "acq_date_time_offset = scripts_dir  /'acquisition_date_time_offset.pklz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda environment used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda env export > {scripts_dir.joinpath('conda_env.yml')} \n",
    "# this can be subsequently edited to remove some package versions that don't exist on osx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conda environment used for this analysis can be recreated using the above yml file and the command :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda env create -f conda_env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example provides an example in which the required dataframe \"df_dicoms\" is created. The paths to all dicoms are contained in the column \"dicom_path\" and the patient id and  date of scan is extracted from the scan name. The date is used to sort the scans so that the session numbers assigned in the next cell are in the same order as the scans are acquired. Custom code to generate the subject and session ids can of course replace these two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dicoms = pd.DataFrame({'dicom_path' : [p.as_posix() for p in dicoms_dir.glob('*' + dicom_extension)]})\n",
    "df_dicoms = pd.concat([df_dicoms,\n",
    "                       df_dicoms.dicom_path.str.extract(\n",
    "                              '.*-(?P<patient_id>[0-9]{,8})-(?P<date>[0-9]{8})-*-.*.tgz',expand=True)],\n",
    "                       axis = 1)\n",
    "\n",
    "df_dicoms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bids_ses(df):\n",
    "    df = df.assign(bids_ses = ['{num:02d}'.format(num = 1 + i) for i in range(len(df))])\n",
    "    return df\n",
    "\n",
    "  \n",
    "df_bids = df_dicoms.apply(\n",
    "    lambda row: heudiconv_helpers.helpers.gen_bids_subj(\n",
    "        row,patient_key_path,generate_keys=True),axis = 1)\n",
    "df_bids = (df_bids.sort_values(['bids_subj','date']).\n",
    "           groupby(['bids_subj'],as_index = False).\n",
    "           apply(add_bids_ses)\n",
    "           ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_bids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create symlinks for heudiconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways of using heudiconv. The use-pattern described here requires the dicom tars to have the subject and session in the file name. This is handled by creating a directory of symlinks with the appropriate filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids['symlink_names'] = df_bids.apply(lambda row: heudiconv_helpers.helpers.get_symlink_name(row),axis=1)\n",
    "df_bids = df_bids.assign(symlink_path = lambda df: [symlinked_dicoms.joinpath(p) for p in df.symlink_names])\n",
    "df_bids['symlink_template'] = df_bids.apply(hh.make_symlink_template,project_dir_absolute= project_dir_absolute.as_posix(),axis=1)\n",
    "df_bids['dicom_template'] =  df_bids.symlink_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids.apply(lambda row: hh.make_symlink(row,project_dir_absolute,overwrite_previous=True), axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run heudiconv without conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first run of heudiconv, described here as \"generic\", uses a heudiconv heuristics script designed to extract the metadata for all scans in the dicoms. This provides information for subsequent runs during which the heuristics more accurately map each scan onto the desired file layout and naming convention, in our case BIDS. This subsequent run also allows us to be more picky with which scans we choose to use for the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate parallel commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(heudiconv_helpers)\n",
    "df_sing = df_bids\n",
    "df_sing['cmd'] = df_bids.apply(\n",
    "    lambda row:\n",
    "    heudiconv_helpers.helpers.make_heud_call(row = row,\n",
    "                           project_dir = project_dir_absolute,\n",
    "                           output_dir=outdir_gen,\n",
    "                           conversion = False,\n",
    "                           container_image = sing_image,\n",
    "                           bind_path  = \"/gs3,/gs4,/gs5,/gs6,/gs7,/gs8,/gs9,/gs10,/gs11,/spin1,/scratch,/fdb\",\n",
    "                           scratch_dir = \"/lscratch/$SLURM_JOB_ID\"),\n",
    "        axis = 1)\n",
    "\n",
    "\n",
    "conversion_dict['generic'] = (Path(scripts_dir)\n",
    "                              heudiconv_helpers.helpers.joinpath(\n",
    "                                  'heudiconv_generic_swarm_' + analysis_version + '.cmd'))\n",
    "\n",
    "# not all commands resolve to a single dicom so getting unique ones before writing swarm\n",
    "conversion_dict['generic'].write_text('\\n'.join(df_sing.cmd.drop_duplicates())) \n",
    "print(conversion_dict['generic'].read_text().splitlines()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of running the commands on a server:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "task_graph = []\n",
    "for cmd in conversion_dict['generic'].read_text().split('\\n'):\n",
    "    task_graph.append(delayed(run_cmd)(cmd))\n",
    "\n",
    "result = client.compute(task_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of running the command on nih cluster:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "job_id_gen = !swarm -f {conversion_dict['generic']} -g 10 -t 2 --logdir {conversion_log} --partition quick,nimh --gres=lscratch:30  --time 00:59:00\n",
    "job_id_gen = job_id_gen[0]\n",
    "job_id_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heudiconv with custom heuristics script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heudiconv requires a heuristics file (created in the next section) in order to map the dicom files' metadata to the bids output structure. This is documented at the  nipy/heudiconv github repository. The file contains two main parts:\n",
    "1. Templates create using the \"create_key\" function that specify where each run type belongs\n",
    "2. The specification of the heuristic to categorise each run in the dicom tar.\n",
    "\n",
    "The template is quite stereotyped and the examples on github are useful in figuring out how to write them.\n",
    "\n",
    "The heuristic for categorising the runs is a little more challenging. Often the series description from the dicom header can be enough to categorise the scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example heuristics file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example heuristics file can be a little confusing. The subsequent walks through how to test and debug it though so fear not! Following that section you can fully debug the heuristics specified before you use it with heudiconv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristics_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile anal/heudiconv_files/heuristics/fmrif_heuristics2018_08_28.py\n",
    "# modify the file path entered in the line above to match the \n",
    "# current heuristics script path\n",
    "# sub-<participant_label>[_ses-<session_label>]_task-<task_label>[_acq-<label>][_rec-<label>][_run-<index>][_echo-<index>]_bold.nii[.gz]\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "lgr = logging.getLogger('heudiconv')\n",
    "\n",
    "\n",
    "def infotoids(seqinfos, outdir):\n",
    "    # This is included from reproin. Look there for examples on \n",
    "    # how to use it.\n",
    "    lgr.info(\"Processing sequence infos to deduce study/session\")\n",
    "\n",
    "    subject = get_unique(seqinfos, 'patient_id')\n",
    "    locator = 'none_defined_yet'\n",
    "\n",
    "    return {\n",
    "        # TODO: request info on study from the JedCap\n",
    "        'locator': locator,\n",
    "        # Sessions to be deduced yet from the names etc TODO\n",
    "        'session': 'not_working_yet',\n",
    "        'subject': subject,\n",
    "    }\n",
    "\n",
    "def get_unique(seqinfos, attr):\n",
    "    \"\"\"Given a list of seqinfos, which must have come from a single study\n",
    "    get specific attr, which must be unique across all of the entries\n",
    "\n",
    "    If not -- fail!\n",
    "\n",
    "    \"\"\"\n",
    "    values = set(getattr(si, attr) for si in seqinfos)\n",
    "    assert (len(values) == 1)\n",
    "    return values.pop()\n",
    "\n",
    "def filter_dicom(dcmdata):\n",
    "    \"\"\"Return True if a DICOM dataset should be filtered out, else False\"\"\"\n",
    "    comments = getattr(dcmdata, 'ImageComments', '')\n",
    "    if len(comments):\n",
    "        if 'reference volume' in comments.lower():\n",
    "            print(\"Filter out image with comment '%s'\" % comments)\n",
    "            return True\n",
    "    return False\n",
    "    # Another format:return True if dcmdata.StudyInstanceUID in dicoms2skip else False\n",
    "\n",
    "\n",
    "def filter_files(fn):\n",
    "    \"\"\"\n",
    "    This is used by heudiconv to filter files based on the filename.\n",
    "    The function returns a boolean for a given filename.\n",
    "    \"\"\"\n",
    "    patterns_to_filter_out = ['README',\n",
    "                              'requisition',\n",
    "                              'realtime',\n",
    "                              'edti',\n",
    "                              'optional',\n",
    "                              '3 plane loc',\n",
    "                              'ASSET cal',\n",
    "                              'clinical',\n",
    "                              'plane_loc',\n",
    "                             'nihpcasl']\n",
    "#                               'rest_assetEPI']\n",
    "    return all(pat not in fn for pat in patterns_to_filter_out)\n",
    "\n",
    "\n",
    "def create_key(template, outtype=('nii.gz'), annotation_classes=None):\n",
    "    if template is None or not template:\n",
    "        raise ValueError('Template must be a valid format string')\n",
    "    return template, outtype, annotation_classes\n",
    "\n",
    "\n",
    "def infotodict(seqinfo, test_heuristics=False):\n",
    "    ## sometimes seqinfo is a list of seqinfo objects, sometimes it is a seqinfo object\n",
    "    if not hasattr(seqinfo,'keys'):\n",
    "        seqinfo_dict = {'no_grouping' : seqinfo}\n",
    "    else:\n",
    "        seqinfo_dict = seqinfo\n",
    "        \n",
    "    mprage = create_key(\n",
    "        'sub-{subject}/{session}/anat/sub-{subject}_{session}_acq-mprage_run-{item:03d}_T1w')\n",
    "#     dti = create_key(\n",
    "#         'sub-{subject}/{session}/dwi/sub-{subject}_{session}_run-{item:03d}_dwi')\n",
    "    rest = create_key(\n",
    "        'sub-{subject}/{session}/func/sub-{subject}_{session}_task-rest_run-{item:03d}_bold')\n",
    "#     audition = create_key(\n",
    "#         'sub-{subject}/{session}/func/sub-{subject}_{session}_task-audition_run-{item:03d}_bold')\n",
    "\n",
    "\n",
    "    info = {mprage: [],\n",
    "            rest: [],\n",
    "#             audition: [],\n",
    "           }\n",
    "    heurs = {\n",
    "\n",
    "        \"('MPRAGE' in seq.series_description.upper().replace('_','').replace('-','').replace(' ',''))\": \"info[mprage].append([seq.series_id])\",\n",
    "\n",
    "        \"('rest_assetEPI' in seq.series_description)\": \"info[rest].append([seq.series_id])\",\n",
    "#         \"('Audition fmri' in seq.series_description)\": \"info[audition].append([seq.series_id])\",\n",
    "\n",
    "#         \"('edti_cdiflist' in seq.series_description)\": \"info[dti].append([seq.series_id])\",\n",
    "    }\n",
    "    \n",
    "    if test_heuristics:\n",
    "        for group_id, seqinfo in seqinfo_dict.items():\n",
    "            for seq in seqinfo:\n",
    "                for criterion, action in heurs.items():\n",
    "                    eval(criterion)\n",
    "                    eval(action)\n",
    "                print(\"The defined heuristics evaluate\")\n",
    "                return None\n",
    "    for group_id, seqinfo in seqinfo_dict.items():\n",
    "        if len(seqinfo) > 30:\n",
    "            print(\"There are a lot of entries provided here (%s).\" \n",
    "                  \" This heuristic file does not handle duplicate\"\n",
    "                  \" series_id across the same accession_number.\"\n",
    "                  \" This can be avoided by passing subject/session\"\n",
    "                  \" combinations individually to heudiconv\"% len(seqinfo))\n",
    "        for seq in seqinfo:\n",
    "            for criterion, action in heurs.items():\n",
    "                if eval(criterion):\n",
    "                    eval(action)\n",
    "                    break\n",
    "        #\n",
    "\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm heuristic file fully evaluates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heudiconv_helpers.helpers.dry_run_heurs(heuristics_script=heuristics_script,test_heuristics=True)\n",
    "# ipdb.runcall(heudiconv_helpers.helpers.dry_run_heurs,heuristics_script=heuristics_script,seqinfo=seqinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the output of a heuristics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(heudiconv_helpers.helpers)\n",
    "val = heudiconv_helpers.helpers.validate_heuristics_output(heuristics_script=heuristics_script,verbose = True)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the application of heuristics to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If using a specific heudiconv run use this:\n",
    "dir_to_use = output_of_heudiconv_gen\n",
    "# dir_to_use = Path('./bids_2017_07_14_generic/')\n",
    "\n",
    "info_text_paths_gen = [x for x in dir_to_use.glob('**/info/*tsv')]\n",
    "if len(info_text_paths_gen) == 0:\n",
    "    info_text_paths_gen = [x for x in dir_to_use.glob('**/info/dicom*txt')]\n",
    "if len(info_text_paths_gen) == 0:\n",
    "    raise ValueError(\"Cannot find any info files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_gen = pd.concat([pd.read_csv(p, sep = '\\t').assign(file_path=p) for p in info_text_paths_gen]).reset_index(drop = True)\n",
    "\n",
    "df_info_mapping = pd.concat(\n",
    "    [\n",
    "        heudiconv_helpers.helpers.dry_run_heurs(\n",
    "            heuristics_script=heuristics_script,\n",
    "            seqinfo=list(df.itertuples())) for x,df in df_info_gen.groupby('file_path') ],\n",
    "axis = 0)\n",
    "df_info_mapping = df_info_mapping.assign(file_path = lambda df: df.file_path.astype(str))\n",
    "df_info_mapping = df_info_mapping.assign(participant_id = lambda df: 'sub-' + df.file_path.str.extract('.*.heudiconv/(?P<subj>\\d{4})/.*', expand = True))\n",
    "df_info_mapping.to_csv(info_mapping_csv,index=False,sep=',')\n",
    "df_info_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code for debugging\n",
    "# reload(heudiconv_helpers.helpers)\n",
    "# for x, df in df_info_gen.groupby('file_path'):\n",
    "#     seqinfo = list(df.itertuples())\n",
    "#     seqinfo_dict = {x : seqinfo}\n",
    "# break\n",
    "# heudiconv_helpers.helpers.dry_run_heurs(heuristics_script=heuristics_script,seqinfo=seqinfo_dict)\n",
    "# ipdb.runcall(heudiconv_helpers.helpers.dry_run_heurs,\n",
    "#              heuristics_script=heuristics_script,\n",
    "#              seqinfo=seqinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# df.query('template.notnull() and template.str.contains(\"fmap\")')\n",
    "# df.query('example_dcm_file.str.contains(\"READM\")')\n",
    "# df.series_description.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate heudiconv swarm commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(heudiconv_helpers.helpers)\n",
    "df_sing = df_bids\n",
    "df_sing['cmd'] = df_bids.apply(\n",
    "    lambda row:\n",
    "    heudiconv_helpers.helpers.make_heud_call(row = row,\n",
    "                           project_dir = project_dir_absolute,\n",
    "                           output_dir=outdir,\n",
    "                           conversion = True,\n",
    "                           container_image = sing_image,\n",
    "                           use_scratch = True,\n",
    "                            grouping = 'studyUID',\n",
    "                           heuristics_script = heuristics_script,\n",
    "                          dev_dir = heudiconv_source_code,\n",
    "                          dev = True\n",
    "                      \n",
    "                            ),\n",
    "        axis = 1)\n",
    "\n",
    "run_type = 'conversion'\n",
    "conversion_dict[run_type] = Path(scripts_dir).joinpath(f'heudiconv_{run_type}_{analysis_version}.cmd')\n",
    "\n",
    "# not all commands resolve to a single dicom so getting unique ones before writing swarm\n",
    "conversion_dict[run_type].write_text('\\n'.join(df_sing.cmd.drop_duplicates())) \n",
    "print([x for x in conversion_dict[run_type].read_text().splitlines()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(df_sing.cmd),len(df_sing.cmd.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run heudiconv conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of running the commands on a server:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "task_graph = []\n",
    "for cmd in conversion_dict[run_type].read_text().split('\\n'):\n",
    "    task_graph.append(delayed(run_cmd)(cmd))\n",
    "\n",
    "result = client.compute(task_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example of running the commands on nih cluster:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "job_id = !swarm -f {conversion_dict[run_type]} -g 10 --logdir {conversion_log}  --partition quick,nimh --gres=lscratch:30  --time 01:40:00\n",
    "job_id = job_id[0]\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ways of processing the data when uploading to public repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing participants tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bids\n",
    "layout = bids.BIDSLayout(output_of_heudiconv.as_posix())\n",
    "df_pybids = layout.as_data_frame().query('path.str.contains(\"nii.gz\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pybids['json_path'] = (\n",
    "    df_pybids.path.apply(\n",
    "        lambda x: Path(''.join([*x.split('.')[:-2], '.json']))))\n",
    "\n",
    "df_pybids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove ttl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_dir = Path('conversion_provenance')\n",
    "if not prov_dir.exists(): prov_dir.mkdir()\n",
    "!find bids_2018_08 -name '*ttl' -exec mv {} conversion_provenance \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_file = output_of_heudiconv.joinpath('dataset_description.json')\n",
    "description_file.touch()\n",
    "description_file.write_text(\"\"\"\\\n",
    "{\n",
    "    \"Name\": \"NNDSP Data\",\n",
    "    \"BIDSVersion\": \"1.0.1\",\n",
    "    \"Authors\": [\n",
    "        \"John A. Lee\",\n",
    "        \"Dylan Nielson\",\n",
    "        \"Adam Thomas\"\n",
    "    ],\n",
    "    \"Funding\": \"NIMH Intramural Research Program\"\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {'rest' : 'Resting State'}\n",
    "for k,v in task_dict.items():\n",
    "\n",
    "    json_path = list(output_of_heudiconv.glob('task*' + k + '*.json'))\n",
    "    assert len(json_path) == 1\n",
    "    json_path = json_path[0]\n",
    "    row = pd.Series({'json_path' : json_path})\n",
    "    heudiconv_helpers.helpers.json_action(row,\n",
    "                                          fieldnames = ['TaskName'],\n",
    "                                          action = 'set',\n",
    "                                          values_to_set = [v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile path_to_bids_directory/README\n",
    "# Overview\n",
    "This dataset is released as part of the NIMH/NHGRI Data-Sharing Project (NNDSP). \n",
    "\n",
    "# Summary\n",
    "The dataset contains 441 subjects (188 females) with an age range of 5-77 years old (median = 20 years). At least one anatomical MPRAGE T1w scan was collected for each subject with a total of 490 scans. Each anatomical scan was rated from 1 to 4 with a score of 3 or more being considered unusable as described by Blumenthal et al. (2002). Using this threshold, 32  of the 487 scans were rated as bad by the human raters. Resting state fMRI scans were collected for over 80% of the subjects and task-based fMRI scans (an audition task) were collected in over 30% of the subjects.\n",
    "\n",
    "## Data Acquisition\n",
    "Data was acquired on GE's 3T, Signa_HDxt and Signa_HDx scanners\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_of_heudiconv.joinpath('dataset_description.json').read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If sharing the dataset broadly this is a good time to deface the relevant scans..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove scans with bad metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jitter scan date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scans_tsv(series,path_col):\n",
    "    scans_tsv = Path(series[path_col]).parent.parent.joinpath('sub-' + series.subject + '_' + 'ses-' + series.session + '_scans.tsv')\n",
    "    \n",
    "    return scans_tsv\n",
    "\n",
    "df_pybids['scans_tsv_path'] = (\n",
    "    df_pybids.\n",
    "                apply(lambda df:\n",
    "                      get_scans_tsv(df,'path'),\n",
    "                      axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offset = (heudiconv_helpers.helpers.gen_subj_time_jitter(\n",
    "    df_pybids['subject'].\n",
    "    dropna().\n",
    "    drop_duplicates().\n",
    "    values,\n",
    "    acq_date_time_offset)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsvs =  df_pybids.drop_duplicates('scans_tsv_path',keep='first')\n",
    "df_tsvs.apply(lambda row: \n",
    "              heudiconv_helpers.helpers.rewrite_tsv(\n",
    "                  row['scans_tsv_path'],\n",
    "                  df_offset,row['subject'],\n",
    "                            dry_run=False),\n",
    "              axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove events files for rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_arg = '{}'\n",
    "!find {output_of_heudiconv} -name '*rest*events*' -exec rm {find_arg} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove some json fields including ones containing PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pybids = heudiconv_helpers.helpers.get_bids_df(output_of_heudiconv,scans_only=True)\n",
    "df_pybids['participant_id'] = 'sub-' + df_pybids.subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !chmod -R 770 {output_of_heudiconv}\n",
    "fieldname_tuples = [\n",
    "    ('global','const','AccessionNumber'),\n",
    "    ('global','const','RequestAttributesSequence'),\n",
    "    ('global','const','SeriesTime'),\n",
    "    ('global','const','StudyID'),\n",
    "    ('global','const','StudyTime'),\n",
    "    ('global','const','ContentTime'),\n",
    "    'AcquisitionDateTime',\n",
    "    'InstitutionAddress']\n",
    "\n",
    "df_pybids.apply(\n",
    "    lambda row: \n",
    "    heudiconv_helpers.helpers.json_action(\n",
    "        row,fieldnames = fieldname_tuples,action='delete'),\n",
    "    axis = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_files = heudiconv_helpers.helpers.get_bids_df(output_of_heudiconv)\n",
    "for x,df in df_all_files.query(f'path.str.contains(\"{output_of_heudiconv}/task-\")').iterrows():\n",
    "    (heudiconv_helpers.helpers.\n",
    "     json_action(\n",
    "         df,action='delete',json_col='path', fieldnames= [\"CogAtlasID\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bids validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(heudiconv_helpers.helpers)\n",
    "print(heudiconv_helpers.helpers.validate_bids_dir(output_of_heudiconv,verbose=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = output_of_heudiconv / '.heudiconv'\n",
    "if hidden.exists():\n",
    "    target = project_dir_absolute.joinpath(f'.{analysis_version}_conversion')\n",
    "    if not target.exists():\n",
    "        target.mkdir()\n",
    "    !mv {hidden} {target}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos = !find {output_of_heudiconv} -name '*.json' |xargs grep TODO\n",
    "if todos:\n",
    "    print(todos)\n",
    "    print(\"Some of the files contain TODOs\")\n",
    "    \n",
    "if any(\n",
    "    [Path(output_of_heudiconv).joinpath('.heudiconv').exists(),\n",
    "    Path('bids/sourcedata').exists()]):\n",
    "    print('move files')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
